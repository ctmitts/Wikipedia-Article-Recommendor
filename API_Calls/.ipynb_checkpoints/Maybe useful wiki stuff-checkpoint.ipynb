{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_category( category, sub_category = False):  ## return\n",
    "    '''Request from wikipedia API for category pages (articles, subcategories) \n",
    "            return:  JSON object with category pages  '''\n",
    "    if sub_category:\n",
    "        cat_tag = '&cmtitle=Category:' + sub_category ## append category to cat_tag\n",
    "    else: \n",
    "        cat_tag = '&cmtitle=Category:' + category ## append category to cat_tag\n",
    "    \n",
    "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    action_tag = \"?action=query&list=categorymembers&cmlimit=max\" ## fetch all category members (pages, subcategories)\n",
    "    #category_tag =  cat_tag#'&cmtitle=Category:' + category ## append category to cat_tag\n",
    "    parameters_tag = \"&format=json&prop=info|categories|links\" #&prop=categories|links|info\" ## return in json format\n",
    "    request_call = base_url + action_tag + cat_tag + parameters_tag ## concatenate base_url with request tags\n",
    "    request = requests.get( request_call)  ## request HTTP results\n",
    "    \n",
    "    category_pages = request.json()['query']['categorymembers']  ## list object containing category pages ( articles, sub-categories)\n",
    "    \n",
    "    pages = {}\n",
    "    for i, page_info in enumerate( category_pages):\n",
    "        page_id = page_info['pageid']\n",
    "        page_title = page_info['title']\n",
    "        if sub_category:\n",
    "            pages[i] ={'category': category, 'sub-category': sub_category, 'pageid': page_id, 'title': page_title, 'content':np.nan } \n",
    "        else:\n",
    "            pages[i] ={'category': category, 'sub-category': None, 'pageid': page_id, 'title': page_title, 'content':np.nan } \n",
    "    \n",
    "    pages_df = pd.DataFrame.from_dict( pages, orient = 'index')\n",
    "    \n",
    "    pages_df['title']\n",
    "    \n",
    "    return pages_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_content( page_id):\n",
    "    try:\n",
    "        page_content = wikipedia.WikipediaPage(pageid = page_id).content\n",
    "    except: \n",
    "        page_content = ''\n",
    "    return page_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_content( page_df, condition = True):\n",
    "    \n",
    "    category = page_df.category.unique()[0]\n",
    "\n",
    "    sub_category_mask = page_df.title.str.contains( 'Category:')  ## row mask \n",
    "    \n",
    "    new_articles_mask = ~sub_category_mask & page_df.content.isnull()\n",
    "     \n",
    "    sub_category_indices = page_df[sub_category_mask].index.tolist()\n",
    "    ## gather all the article content for the each page in the category, excluding the sub_categories\n",
    "    page_df.loc[new_articles_mask, 'content'] = page_df[ new_articles_mask ].apply( lambda x: grab_content( x.pageid), axis = 1 )\n",
    "    \n",
    "    n_sub_categories = sum( sub_category_mask)\n",
    "    if n_sub_categories == 0:\n",
    "        condition = False\n",
    "        return page_df\n",
    "    \n",
    "    \n",
    "    while condition:\n",
    "        \n",
    "        subCat_indice = sub_category_indices[0] ## grab the first one\n",
    "        \n",
    "        \n",
    "        subCat = page_df.iloc[subCat_indice,:]['title'].split('Category:')[1] # subCat = \n",
    "        \n",
    "        page_df = page_df.append( request_category( category, subCat ), ignore_index= True)\n",
    "        \n",
    "        \n",
    "        page_df.drop( page_df.index[ subCat_indice], inplace = True )  ## Remove the original page\n",
    "        \n",
    "        gather_content( page_df)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## New Gather Content:\n",
    "\n",
    "def gather_articles( category, article_categories = False):\n",
    "    '''Collect all articles that belong to a category, including articles found in sub-categories belonging to category\n",
    "        that are not already included under the category explicitly'''\n",
    "    category_pages_df = request_category( category)  ## load dataframe of pages contained in category\n",
    "    \n",
    "    sub_category_mask = category_pages_df.title.str.contains( 'Category:')  ## row mask for only sub-categories\n",
    "    n_sub_categories = sum( sub_category_mask)  ## Number of sub-categories belong to category\n",
    "    ## Create list of all sub-category names ( Category: <name>), remove the preface          \n",
    "    sub_category_list = category_pages_df[ sub_category_mask].title.replace( 'Category:', '').tolist()  \n",
    "    \n",
    "    if article_categories:\n",
    "        ## Create list of all pageids \n",
    "        article_categories = {}\n",
    "        article_id_list = category_pages_df[ ~sub_category_mask].pageid.tolist()  ## article pageids\n",
    "    \n",
    "        for article_id in article_id_list:\n",
    "            if article_id in article_categories:\n",
    "                article_categories[article_id].append( category)\n",
    "            if article not in article_categories:\n",
    "                article_categories[ article_id ] = [category]\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pages_df_list = []\n",
    "    \n",
    "    pages_df = category_pages_df[ ~sub_category_mask]\n",
    "    \n",
    "    pages_df_list.append( pages_df)\n",
    "    \n",
    "    if num_sub_categories > 0:\n",
    "\n",
    "        for sub_category in sub_category_list:  ## recursively, one at a time\n",
    "            \n",
    "            pages_df_list.append( gather_articles( sub_category, article_categories) )\n",
    "            \n",
    "\n",
    "   \n",
    "    pages_df = pd.concat( pages_df_list)\n",
    "    \n",
    "    \n",
    "    pages_df.reset_index()\n",
    "    \n",
    "    return pages_df\n",
    "    \n",
    "    \n",
    "    #pages_df_list.append(  category_pages_df[~sub_category_mask].pageid.apply( grab_content ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
